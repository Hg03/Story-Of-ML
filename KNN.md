## K Nearest Neighbors Intuition

**K Nearest Neighbors** ğŸ§‘â€ğŸ¦¯ is a Supervised Machine Learning Algorithm which helps in both classification and regression tasks. **KNN** algorithm is different from other algorithms like **Linear Regression**, **Logistic Regression**, **Support Vector Machine** tries to create a decision boundary in form of line, plane or some hyperplane but KNN tries to predict the class (target) of new sample on the basis of its surrounding datapoints using **Euclidena**/**Manhattan** Distance. Let's look into it in more detail.

### How KNN is implemented ?? ğŸ¤” ğŸ’­

Imagine we have a data having features **IMDB ratings**, **Box office collection**, **PG ratings**

|IMDB Ratings|Box Office Collection|PG Ratings|
|------------|---------------------|----------|
|55|10|ğŸŸ¡|
|60|20|ğŸŸ¢|
|57|22|ğŸŸ¡|
|61|17|ğŸŸ¢|
|65|16|ğŸ”´|
|69|45|ğŸ”´|
|75|32|ğŸŸ¢|
|69|43|ğŸŸ¡|
|..|..|..|
|..|..|..|

and so on ğŸŸ¢ğŸŸ¡ğŸ”´

Here ğŸŸ¢ is for **PG** rated, ğŸŸ¡ is for **PG13** rated and ğŸ”´ is for **R** rated. Let's plot the datapoints.

<img src="https://github.com/Hg03/Story-Of-ML/blob/main/assets/ratingplotted.png">

