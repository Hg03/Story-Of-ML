## K Nearest Neighbors Intuition

**K Nearest Neighbors** ğŸ§‘â€ğŸ¦¯ is a Supervised Machine Learning Algorithm which helps in both classification and regression tasks. **KNN** algorithm is different from other algorithms like **Linear Regression**, **Logistic Regression**, **Support Vector Machine** tries to create a decision boundary in form of line, plane or some hyperplane but KNN tries to predict the class (target) of new sample on the basis of its surrounding datapoints using **Euclidena**/**Manhattan** Distance. Let's look into it in more detail.

### How KNN is implemented ?? ğŸ¤” ğŸ’­

Imagine we have a data having features **IMDB ratings**, **Box office collection**, **PG ratings**

|IMDB Ratings|Box Office Collection|PG Ratings|
|------------|---------------------|----------|
|55|10|ğŸŸ¡|
|60|20|ğŸŸ¢|
|57|22|ğŸŸ¡|
|61|17|ğŸŸ¢|
|65|16|ğŸ”´|
|69|45|ğŸ”´|
|75|32|ğŸŸ¢|
|69|43|ğŸŸ¡|
|..|..|..|
|..|..|..|

and so on ğŸŸ¢ğŸŸ¡ğŸ”´

Here ğŸŸ¢ is for **PG** rated, ğŸŸ¡ is for **PG13** rated and ğŸ”´ is for **R** rated. Let's plot the datapoints.

<img src="https://github.com/Hg03/Story-Of-ML/blob/main/assets/ratingplotted.png">

As we can , it's very difficult to draw a decision boundary becuase data is so much random, there **KNN** is very best suited algorithm here. Let's say we encountered a new sample, we'll select some **K** value. Let's say K = 4, therefore we select 4 nearest datapoints from a new sample and check which majority target class those nearest datapoints favours :
1. If 2 datapoints favour ğŸ”´, 1 favours ğŸŸ¡ and 1 favours ğŸŸ¢ depicts that new sample belongs to ğŸ”´ class.
2. If 3 datapoints favour ğŸŸ¡ and 1 favours ğŸŸ¢ depicts that new sample belongs to ğŸŸ¡ class.
3. If 2 datapoints favour ğŸ”´, 2 favours ğŸŸ¡ , then we'll calculate the average distance of ğŸ”´ class as well as for ğŸŸ¡ class. Then new sample belongs to those class whose class' average is lesser.



